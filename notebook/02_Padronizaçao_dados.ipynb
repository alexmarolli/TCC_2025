{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T00:15:22.780651Z",
     "start_time": "2025-11-03T00:15:22.077309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from config import LIST_TICKETS\n",
    "\n",
    "print(f\"A√ß√µes dispon√≠veis para treino: {LIST_TICKETS}\")\n",
    "# --- Configura√ß√µes ---\n",
    "TICKER_PARA_PROCESSAR = LIST_TICKETS\n",
    "# CORRE√á√ÉO APLICADA AQUI üëá\n",
    "CAMINHO_DADOS_BRUTOS = \"../data/01_raw\"\n",
    "CAMINHO_DADOS_PROCESSADOS = \"../data/03_processed\"\n",
    "CAMINHO_MODELOS = \"../models\"\n",
    "\n",
    "# Par√¢metros para o pr√©-processamento\n",
    "PERCENTUAL_TREINO = 0.8\n",
    "JANELA_DE_TEMPO = 60 # Usaremos 60 dias de hist√≥rico para prever o pr√≥ximo\n",
    "\n",
    "# Garante que os diret√≥rios de sa√≠da existam\n",
    "os.makedirs(CAMINHO_DADOS_PROCESSADOS, exist_ok=True)\n",
    "os.makedirs(CAMINHO_MODELOS, exist_ok=True)\n",
    "\n",
    "print(\"Configura√ß√µes carregadas.\")"
   ],
   "id": "b77a8055a919257d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A√ß√µes dispon√≠veis para treino: ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA']\n",
      "Configura√ß√µes carregadas.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:31.679588Z",
     "start_time": "2025-10-31T03:46:31.632419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constr√≥i o caminho completo para o arquivo CSV\n",
    "caminho_arquivo = os.path.join(CAMINHO_DADOS_BRUTOS, f\"{TICKER_PARA_PROCESSAR}.csv\")\n",
    "\n",
    "# Carrega os dados, pulando as linhas de cabe√ßalho extras se necess√°rio\n",
    "# (Ajuste 'header' se os arquivos tiverem formatos diferentes)\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        caminho_arquivo,\n",
    "        header=2, # Assumindo que o arquivo baixado pelo yfinance tem cabe√ßalho na primeira linha\n",
    "        index_col='Date',\n",
    "        parse_dates=True\n",
    "    )\n",
    "    print(f\"Dados de {TICKER_PARA_PROCESSAR} carregados com sucesso.\")\n",
    "    print(\"Primeiras 5 linhas:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: Arquivo n√£o encontrado em '{caminho_arquivo}'\")\n",
    "\n",
    "# Tratamento de valores nulos (se houver)\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"\\nValores nulos removidos.\")"
   ],
   "id": "4f59972086322454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de PETR4.SA carregados com sucesso.\n",
      "Primeiras 5 linhas:\n",
      "            Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4   Unnamed: 5\n",
      "Date                                                                   \n",
      "2000-01-03    1.156394    1.156394    1.156394    1.156394  35389440000\n",
      "2000-01-04    1.092423    1.092423    1.092423    1.092423  28861440000\n",
      "2000-01-05    1.081401    1.081401    1.081401    1.081401  43033600000\n",
      "2000-01-06    1.077661    1.077661    1.077661    1.077661  34055680000\n",
      "2000-01-07    1.082582    1.082582    1.082582    1.082582  20912640000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:31.734149Z",
     "start_time": "2025-10-31T03:46:31.727221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Seleciona a coluna 'Close' e a transforma em um array NumPy\n",
    "dados_fechamento = df.iloc[:,1].values.reshape(-1, 1)\n",
    "\n",
    "# Calcula o ponto de divis√£o entre treino e teste\n",
    "ponto_divisao = int(len(dados_fechamento) * PERCENTUAL_TREINO)\n",
    "\n",
    "# Separa os dados\n",
    "dados_treino = dados_fechamento[:ponto_divisao]\n",
    "dados_teste = dados_fechamento[ponto_divisao:]\n",
    "\n",
    "print(f\"Tamanho total dos dados: {len(dados_fechamento)}\")\n",
    "print(f\"Tamanho do conjunto de treino: {len(dados_treino)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(dados_teste)}\")"
   ],
   "id": "26ac2cad0ee60b9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho total dos dados: 6487\n",
      "Tamanho do conjunto de treino: 5189\n",
      "Tamanho do conjunto de teste: 1298\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:31.809729Z",
     "start_time": "2025-10-31T03:46:31.798185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializa o normalizador (scaler) para a escala de 0 a 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# AJUSTA o scaler APENAS com os dados de TREINO\n",
    "scaler.fit(dados_treino)\n",
    "\n",
    "# TRANSFORMA ambos os conjuntos de dados (treino e teste) com o scaler j√° ajustado\n",
    "dados_treino_normalizados = scaler.transform(dados_treino)\n",
    "dados_teste_normalizados = scaler.transform(dados_teste)\n",
    "\n",
    "print(\"Dados normalizados com sucesso.\")\n",
    "print(\"Primeiros 5 valores do treino normalizado:\", dados_treino_normalizados[:5].flatten())\n",
    "\n",
    "# Salva o scaler para uso futuro (na etapa de previs√£o)\n",
    "caminho_scaler = os.path.join(CAMINHO_MODELOS, f\"{TICKER_PARA_PROCESSAR}_scaler.pkl\")\n",
    "joblib.dump(scaler, caminho_scaler)\n",
    "print(f\"\\nScaler salvo em: {caminho_scaler}\")"
   ],
   "id": "b980aae4bbf34538",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados normalizados com sucesso.\n",
      "Primeiros 5 valores do treino normalizado: [0.02262052 0.01746597 0.0165778  0.01627645 0.01667299]\n",
      "\n",
      "Scaler salvo em: ../models/PETR4.SA_scaler.pkl\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f3685bc094c96c18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:31.866996Z",
     "start_time": "2025-10-31T03:46:31.841292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def criar_sequencias(dados, janela_de_tempo):\n",
    "    X, y = [], []\n",
    "    for i in range(janela_de_tempo, len(dados)):\n",
    "        X.append(dados[i-janela_de_tempo:i, 0])\n",
    "        y.append(dados[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Cria as sequ√™ncias para os dados de treino e teste\n",
    "X_train, y_train = criar_sequencias(dados_treino_normalizados, JANELA_DE_TEMPO)\n",
    "X_test, y_test = criar_sequencias(dados_teste_normalizados, JANELA_DE_TEMPO)\n",
    "\n",
    "# Ajusta o formato de X para ser [amostras, janelas, features], como o LSTM espera\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(\"\\nSequ√™ncias criadas com sucesso.\")\n",
    "print(\"Formato de X_train:\", X_train.shape)\n",
    "print(\"Formato de y_train:\", y_train.shape)\n",
    "print(\"Formato de X_test:\", X_test.shape)\n",
    "print(\"Formato de y_test:\", y_test.shape)"
   ],
   "id": "b81a28a5b392a81f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequ√™ncias criadas com sucesso.\n",
      "Formato de X_train: (5129, 60, 1)\n",
      "Formato de y_train: (5129,)\n",
      "Formato de X_test: (1238, 60, 1)\n",
      "Formato de y_test: (1238,)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:31.935463Z",
     "start_time": "2025-10-31T03:46:31.914031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Salva os arrays processados para n√£o precisar refazer este notebook toda vez\n",
    "np.save(os.path.join(CAMINHO_DADOS_PROCESSADOS, f'{TICKER_PARA_PROCESSAR}_X_train.npy'), X_train)\n",
    "np.save(os.path.join(CAMINHO_DADOS_PROCESSADOS, f'{TICKER_PARA_PROCESSAR}_y_train.npy'), y_train)\n",
    "np.save(os.path.join(CAMINHO_DADOS_PROCESSADOS, f'{TICKER_PARA_PROCESSAR}_X_test.npy'), X_test)\n",
    "np.save(os.path.join(CAMINHO_DADOS_PROCESSADOS, f'{TICKER_PARA_PROCESSAR}_y_test.npy'), y_test)\n",
    "\n",
    "print(f\"\\nDados processados para {TICKER_PARA_PROCESSAR} foram salvos na pasta '{CAMINHO_DADOS_PROCESSADOS}'\")"
   ],
   "id": "a0c1dc5ff6b95486",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados processados para PETR4.SA foram salvos na pasta '../data/03_processed'\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
